import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient
import time
import random
import re

MONGO_URI = "mongodb+srv://lumconon0911:SWD-SWD@cluster.slolqwf.mongodb.net/"
DATABASE_NAME = "topdev_jobs_db"
COLLECTION_NAME = "job_details"


def has_all_classes(tag, class_string):
    if not tag or not tag.has_attr('class'):
        return False
    target_classes_list = class_string.split()
    return all(cls in tag['class'] for cls in target_classes_list)


def get_html_content(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"L·ªói khi t·∫£i trang web '{url}': {e}")
        return None


def extract_detail_links(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    unique_detail_links = set()
    base_url = "https://vieclam24h.vn"  # Th√™m base_url v√†o ƒë√¢y

    target_classes_listing_page = "grid grid-cols-1 gap-y-2 lg:gap-y-2.5"

    container = soup.find('div',
                          class_=lambda c: has_all_classes(BeautifulSoup(f'<div class="{c}"></div>', 'html.parser').div,
                                                           target_classes_listing_page) if c else False)

    if container:
        links = container.find_all('a', href=True)
        for link in links:
            href = link['href']

            # --- THAY ƒê·ªîI ƒê√É TH√äM ·ªû ƒê√ÇY ---
            # N·∫øu href ch·ª©a t·ª´ "page" (kh√¥ng ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng), b·ªè qua.
            if "page" in href.lower():
                print(f"B·ªè qua li√™n k·∫øt ph√¢n trang: {href}")
                continue
            # --- H·∫æT THAY ƒê·ªîI ---

            if href.startswith('http'):
                unique_detail_links.add(href)
            else:
                full_link = f"{base_url}{href}"
                unique_detail_links.add(full_link)
    else:
        print(f"Kh√¥ng t√¨m th·∫•y container v·ªõi c√°c class: '{target_classes_listing_page}'.")

    return unique_detail_links


def extract_section_content(soup_obj, heading_text):
    lower_heading_text = heading_text.lower()

    heading_tag = None
    for tag_name in ['h2', 'h3', 'span']:
        heading_tag = soup_obj.find(tag_name, string=lambda text: text and lower_heading_text in text.lower())
        if heading_tag:
            break

    if heading_tag:
        content_element = None
        specific_content_class = "jsx-5b2773f86d2f74b mb-2 text-14 break-words text-se-neutral-80 text-description"

        content_element = heading_tag.find_next_sibling('div', class_='prose')

        if not content_element:
            for tag_type in ['div', 'span']:
                dummy_tag_html = f'<{tag_type} class="">content</{tag_type}>'
                potential_content = heading_tag.find_next_sibling(tag_type, class_=lambda c: has_all_classes(
                    BeautifulSoup(dummy_tag_html, 'html.parser').find(tag_type),
                    specific_content_class) if c else False)
                if potential_content:
                    content_element = potential_content
                    break

        if not content_element:
            content_element = heading_tag.find_next_sibling(['div', 'ul', 'ol', 'p', 'span'])

        if content_element:
            raw_text = content_element.get_text(separator=' ', strip=False)
            cleaned_text = re.sub(r'\s+', ' ', raw_text).strip()
            return cleaned_text
        else:
            raw_heading_text = heading_tag.get_text(separator=' ', strip=False)
            cleaned_heading_text = re.sub(r'\s+', ' ', raw_heading_text).strip()
            if cleaned_heading_text.lower() == lower_heading_text:
                return "Kh√¥ng t√¨m th·∫•y."
            else:
                return cleaned_heading_text

    return "Kh√¥ng t√¨m th·∫•y."


def process_detail_link(detail_url, collection):
    print(f"\n--- ƒêang x·ª≠ l√Ω li√™n k·∫øt chi ti·∫øt: {detail_url} ---")
    detail_html = get_html_content(detail_url)

    if detail_html:
        print(f"ƒê√£ l·∫•y ƒë∆∞·ª£c n·ªôi dung HTML c·ªßa trang chi ti·∫øt '{detail_url}'.")
        detail_soup = BeautifulSoup(detail_html, 'html.parser')

        job_title = detail_soup.find('title').get_text(strip=True) if detail_soup.find(
            'title') else "Kh√¥ng c√≥ ti√™u ƒë·ªÅ."

        responsibilities_content = extract_section_content(detail_soup, "M√¥ t·∫£ c√¥ng vi·ªác")
        if responsibilities_content == "Kh√¥ng t√¨m th·∫•y.":
            responsibilities_content = extract_section_content(detail_soup, "Responsibilities")

        requirements_content = extract_section_content(detail_soup, "Y√™u c·∫ßu c√¥ng vi·ªác")
        if requirements_content == "Kh√¥ng t√¨m th·∫•y.":
            requirements_content = extract_section_content(detail_soup, "Requirements")

        benefits_content = extract_section_content(detail_soup, "Quy·ªÅn l·ª£i")
        if benefits_content == "Kh√¥ng t√¨m th·∫•y.":
            benefits_content = extract_section_content(detail_soup, "ph√∫c l·ª£i d√†nh cho b·∫°n")
        if benefits_content == "Kh√¥ng t√¨m th·∫•y.":
            benefits_content = extract_section_content(detail_soup, "Benefits")

        # --- KI·ªÇM TRA ƒêI·ªÄU KI·ªÜN L∆ØU V√ÄO DATABASE ---
        # Ch·ªâ l∆∞u v√†o database n·∫øu √≠t nh·∫•t m·ªôt trong ba ph·∫ßn ch√≠nh c√≥ n·ªôi dung kh√°c "Kh√¥ng t√¨m th·∫•y."
        if (responsibilities_content == "Kh√¥ng t√¨m th·∫•y." and
                requirements_content == "Kh√¥ng t√¨m th·∫•y." and
                benefits_content == "Kh√¥ng t√¨m th·∫•y."):
            print(
                f"‚ö†Ô∏è B·ªè qua l∆∞u v√†o database cho '{job_title}' ({detail_url}). Kh√¥ng t√¨m th·∫•y n·ªôi dung M√¥ t·∫£ c√¥ng vi·ªác, Y√™u c·∫ßu c√¥ng vi·ªác, ho·∫∑c Quy·ªÅn l·ª£i.")
            print("-" * (len(detail_url) + 40))
            return  # D·ª´ng h√†m t·∫°i ƒë√¢y, kh√¥ng l∆∞u v√†o database

        job_data = {
            "url": detail_url,
            "title": job_title,
            "responsibilities": responsibilities_content,
            "requirements": requirements_content,
            "benefits": benefits_content,
            "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }

        try:
            result = collection.update_one(
                {'url': job_data['url']},
                {'$set': job_data},
                upsert=True
            )

            if result.upserted_id:
                print(f"‚úÖ ƒê√£ th√™m m·ªõi d·ªØ li·ªáu cho '{job_data['title']}' v√†o MongoDB (ID: {result.upserted_id}).")
            elif result.modified_count > 0:
                print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t d·ªØ li·ªáu cho '{job_data['title']}' trong MongoDB.")
            else:
                print(f"‚ÑπÔ∏è D·ªØ li·ªáu cho '{job_data['title']}' ƒë√£ t·ªìn t·∫°i v√† kh√¥ng c√≥ thay ƒë·ªïi.")

        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u d·ªØ li·ªáu v√†o MongoDB cho '{job_data['title']}': {e}")

        print("\n" + "=" * 50)
        print(f"D·ªÆ LI·ªÜU TR√çCH XU·∫§T T·ª™ TRANG CHI TI·∫æT ({detail_url}):")
        print("=" * 50 + "\n")
        print(f"Ti√™u ƒë·ªÅ: {job_title}")
        print(f"1. M√¥ t·∫£ c√¥ng vi·ªác: {responsibilities_content}")
        print(f"2. Y√™u c·∫ßu c√¥ng vi·ªác: {requirements_content}")
        print(f"3. Quy·ªÅn l·ª£i: {benefits_content}")
        print("\n" + "=" * 50)

    else:
        print(f"Kh√¥ng th·ªÉ t·∫£i n·ªôi dung HTML c·ªßa trang chi ti·∫øt: {detail_url}")
    print("-" * (len(detail_url) + 40))


import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient
import time
import random
import re
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse  # Import necessary modules

# C·∫•u h√¨nh MongoDB Atlas
MONGO_URI = "mongodb+srv://lumconon0911:SWD-SWD@cluster.slolqwf.mongodb.net/"
DATABASE_NAME = "topdev_jobs_db"
COLLECTION_NAME = "job_details"
BASE_URL = "https://vieclam24h.vn"  # Keep this as the base domain

# --- (Rest of your functions: has_all_classes, get_html_content, extract_detail_links, extract_section_content, process_detail_link - these remain unchanged) ---

if __name__ == "__main__":
    client = None
    try:
        client = MongoClient(MONGO_URI)
        client.admin.command('ping')
        print(f"‚úÖ ƒê√£ k·∫øt n·ªëi th√†nh c√¥ng t·ªõi MongoDB Atlas cluster.")
        db = client[DATABASE_NAME]
        collection = db[COLLECTION_NAME]
        print(f"ƒêang s·ª≠ d·ª•ng database '{DATABASE_NAME}' v√† collection '{COLLECTION_NAME}'.")

        # ƒê·∫£m b·∫£o ch·ªâ m·ª•c duy nh·∫•t tr√™n tr∆∞·ªùng 'url' ƒë·ªÉ ngƒÉn ch·∫∑n tr√πng l·∫∑p ·ªü c·∫•p ƒë·ªô DB
        try:
            collection.create_index("url", unique=True)
            print("‚úÖ ƒê√£ ƒë·∫£m b·∫£o ch·ªâ m·ª•c duy nh·∫•t tr√™n tr∆∞·ªùng 'url'.")
        except Exception as e:
            print(f"‚ÑπÔ∏è L·ªói khi t·∫°o ch·ªâ m·ª•c duy nh·∫•t (c√≥ th·ªÉ ƒë√£ t·ªìn t·∫°i): {e}")

        # The initial URL you provided, which includes other parameters
        initial_listing_url = "https://vieclam24h.vn/viec-lam-it-phan-mem-o8.html?occupation_ids[]=8&sort_q=priority_max%2Cdesc"

        all_detail_links = set()  # Use a set to store all unique detail links across pages
        page_number = 1
        max_pages_to_crawl = 3  # Set a reasonable limit for testing (e.g., 3 pages)

        print(f"\n--- B·∫ÆT ƒê·∫¶U CRAWL DANH S√ÅCH VI·ªÜC L√ÄM ---")

        while page_number <= max_pages_to_crawl:
            # Parse the initial URL to get its components
            parsed_url = urlparse(initial_listing_url)
            query_params = parse_qs(parsed_url.query)  # Get query parameters as a dictionary

            # Update the 'page' parameter
            query_params['page'] = [str(page_number)]  # 'parse_qs' returns lists for values, so set it as a list

            # Reconstruct the query string
            new_query_string = urlencode(query_params, doseq=True)  # doseq=True handles lists like occupation_ids[]

            # Reconstruct the full URL for the current page
            current_page_url = urlunparse(parsed_url._replace(query=new_query_string))

            print(f"\nüîé ƒêang truy c·∫≠p trang danh s√°ch: {current_page_url}")

            html_content = get_html_content(current_page_url)

            if html_content:
                detail_links_on_page = extract_detail_links(html_content)

                if not detail_links_on_page:
                    print(
                        f"Kh√¥ng t√¨m th·∫•y li√™n k·∫øt chi ti·∫øt n√†o tr√™n trang {page_number}. C√≥ th·ªÉ ƒë√£ h·∫øt trang ho·∫∑c c·∫•u tr√∫c thay ƒë·ªïi.")
                    break  # No more links found, exit the pagination loop

                new_links_count_on_page = 0
                for link in detail_links_on_page:
                    if link not in all_detail_links:  # Only add if it's a truly new link
                        all_detail_links.add(link)
                        new_links_count_on_page += 1

                print(
                    f"T√¨m th·∫•y {len(detail_links_on_page)} li√™n k·∫øt tr√™n trang {page_number}. Th√™m {new_links_count_on_page} li√™n k·∫øt m·ªõi v√†o danh s√°ch t·ªïng.")

                # If no *new* links were found on this page (and it's not the first page),
                # it's likely we've crawled all available pages or reached the end.
                if new_links_count_on_page == 0 and page_number > 1:
                    print(
                        f"Kh√¥ng t√¨m th·∫•y li√™n k·∫øt m·ªõi n√†o tr√™n trang {page_number}. C√≥ th·ªÉ ƒë√£ thu th·∫≠p t·∫•t c·∫£ ho·∫∑c ƒë·∫øn cu·ªëi.")
                    break

                page_number += 1
                time.sleep(random.uniform(2, 5))  # Be polite, add a delay between list page requests
            else:
                print(
                    f"Kh√¥ng th·ªÉ l·∫•y n·ªôi dung HTML t·ª´ '{current_page_url}' ƒë·ªÉ ph√¢n t√≠ch li√™n k·∫øt. D·ª´ng l·∫°i qu√° tr√¨nh crawl danh s√°ch.")
                break

        print(f"\n--- K·∫æT TH√öC CRAWL DANH S√ÅCH VI·ªÜC L√ÄM ---")

        if all_detail_links:
            print(
                f"\nT·ªïng s·ªë {len(all_detail_links)} li√™n k·∫øt chi ti·∫øt duy nh·∫•t ƒë√£ ƒë∆∞·ª£c t√¨m th·∫•y qua t·∫•t c·∫£ c√°c trang.")
            links_to_process = list(all_detail_links)
            random.shuffle(links_to_process)  # Randomize order for more diverse crawling

            # You can set a limit to process only a subset of links for testing
            max_details_to_process = 50  # Process at most 50 detail links for a test run
            if len(links_to_process) > max_details_to_process:
                print(
                    f"Ch·ªâ x·ª≠ l√Ω {max_details_to_process} li√™n k·∫øt chi ti·∫øt ƒë·∫ßu ti√™n trong s·ªë {len(links_to_process)} t√¨m th·∫•y (thay ƒë·ªïi 'max_details_to_process' ƒë·ªÉ ƒëi·ªÅu ch·ªânh).")
                links_to_process = links_to_process[:max_details_to_process]

            for i, link in enumerate(links_to_process):
                print(f"\n({i + 1}/{len(links_to_process)}) ")  # Show progress
                process_detail_link(link, collection)
                time.sleep(random.uniform(1, 3))  # Delay between detail page requests

        else:
            print("Kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ li√™n k·∫øt chi ti·∫øt n√†o.")
    except Exception as e:
        print(f"‚ùå L·ªói t·ªïng qu√°t trong qu√° tr√¨nh crawl ho·∫∑c k·∫øt n·ªëi MongoDB: {e}")
    finally:
        if client:
            client.close()
            print("\nüëã ƒê√£ ƒë√≥ng k·∫øt n·ªëi MongoDB.")

    print("\nCh∆∞∆°ng tr√¨nh ƒë√£ ho√†n t·∫•t.")