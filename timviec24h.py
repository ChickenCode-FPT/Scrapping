import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient
import time
import random
import re
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

# C·∫•u h√¨nh MongoDB Atlas
MONGO_URI = "mongodb+srv://lumconon0911:SWD-SWD@cluster.slolqwf.mongodb.net/"
DATABASE_NAME = "topdev_jobs_db"
COLLECTION_NAME = "job_details"
BASE_URL = "https://vieclam24h.vn"


def has_all_classes(tag, class_string):
    if not tag or not tag.has_attr('class'):
        return False
    target_classes_list = class_string.split()
    return all(cls in tag['class'] for cls in target_classes_list)


def get_html_content(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"L·ªói khi t·∫£i trang web '{url}': {e}")
        return None


def extract_detail_links(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    unique_detail_links = set()

    target_classes_listing_page = "grid grid-cols-1 gap-y-2 lg:gap-y-2.5"

    container = soup.find('div',
                          class_=lambda c: has_all_classes(BeautifulSoup(f'<div class="{c}"></div>', 'html.parser').div,
                                                           target_classes_listing_page) if c else False)

    if container:
        links = container.find_all('a', href=True)
        for link in links:
            href = link['href']

            if "page" in href.lower():
                # print(f"B·ªè qua li√™n k·∫øt ph√¢n trang: {href}")
                continue

            if href.startswith('http'):
                unique_detail_links.add(href)
            else:
                full_link = f"{BASE_URL}{href}"
                unique_detail_links.add(full_link)
    else:
        print(f"Kh√¥ng t√¨m th·∫•y container v·ªõi c√°c class: '{target_classes_listing_page}'.")

    return unique_detail_links


def extract_section_content(soup_obj, heading_text):
    lower_heading_text = heading_text.lower()

    heading_tag = None
    for tag_name in ['h2', 'h3', 'span']:
        heading_tag = soup_obj.find(tag_name, string=lambda text: text and lower_heading_text in text.lower())
        if heading_tag:
            break

    if heading_tag:
        content_element = None
        specific_content_class = "jsx-5b2773f86d2f74b mb-2 text-14 break-words text-se-neutral-80 text-description"

        content_element = heading_tag.find_next_sibling('div', class_='prose')

        if not content_element:
            for tag_type in ['div', 'span']:
                dummy_tag_html = f'<{tag_type} class="">content</{tag_type}>'
                potential_content = heading_tag.find_next_sibling(tag_type, class_=lambda c: has_all_classes(
                    BeautifulSoup(dummy_tag_html, 'html.parser').find(tag_type),
                    specific_content_class) if c else False)
                if potential_content:
                    content_element = potential_content
                    break

        if not content_element:
            content_element = heading_tag.find_next_sibling(['div', 'ul', 'ol', 'p', 'span'])

        if content_element:
            raw_text = content_element.get_text(separator=' ', strip=False)
            cleaned_text = re.sub(r'\s+', ' ', raw_text).strip()
            return cleaned_text
        else:
            raw_heading_text = heading_tag.get_text(separator=' ', strip=False)
            cleaned_heading_text = re.sub(r'\s+', ' ', raw_heading_text).strip()
            if cleaned_heading_text.lower() == lower_heading_text:
                return "Kh√¥ng t√¨m th·∫•y."
            else:
                return cleaned_heading_text

    return "Kh√¥ng t√¨m th·∫•y."


def normalize_url(url):
    """
    Chu·∫©n h√≥a URL v·ªÅ m·ªôt ƒë·ªãnh d·∫°ng nh·∫•t qu√°n.
    X√≥a c√°c ƒëo·∫°n (fragments), s·∫Øp x·∫øp tham s·ªë truy v·∫•n v√† x√≥a c√°c d·∫•u g·∫°ch ch√©o th·ª´a.
    """
    parsed = urlparse(url)
    path = parsed.path
    query_params = parse_qs(parsed.query, keep_blank_values=True)
    sorted_query = urlencode(sorted(query_params.items()), doseq=True)
    if path.endswith('/') and len(path) > 1:
        path = path.rstrip('/')
    scheme = parsed.scheme.lower()
    netloc = parsed.netloc.lower()
    normalized_url = urlunparse((scheme, netloc, path, parsed.params, sorted_query, ''))
    return normalized_url


def process_detail_link(detail_url, collection):
    normalized_detail_url = normalize_url(detail_url)
    print(f"\n--- ƒêang x·ª≠ l√Ω li√™n k·∫øt chi ti·∫øt: {normalized_detail_url} ---")
    detail_html = get_html_content(normalized_detail_url)

    if detail_html:
        detail_soup = BeautifulSoup(detail_html, 'html.parser')

        job_title = detail_soup.find('title').get_text(strip=True) if detail_soup.find(
            'title') else "Kh√¥ng c√≥ ti√™u ƒë·ªÅ."

        responsibilities_content = extract_section_content(detail_soup, "M√¥ t·∫£ c√¥ng vi·ªác")
        if responsibilities_content == "Kh√¥ng t√¨m th·∫•y.":
            responsibilities_content = extract_section_content(detail_soup, "Responsibilities")

        requirements_content = extract_section_content(detail_soup, "Y√™u c·∫ßu c√¥ng vi·ªác")
        if requirements_content == "Kh√¥ng t√¨m th·∫•y.":
            requirements_content = extract_section_content(detail_soup, "Requirements")

        benefits_content = extract_section_content(detail_soup, "Quy·ªÅn l·ª£i")
        if benefits_content == "Kh√¥ng t√¨m th·∫•y.":
            benefits_content = extract_section_content(detail_soup, "ph√∫c l·ª£i d√†nh cho b·∫°n")
        if benefits_content == "Kh√¥ng t√¨m th·∫•y.":
            benefits_content = extract_section_content(detail_soup, "Benefits")

        # N·∫øu kh√¥ng c√≥ n·ªôi dung ch√≠nh, b·ªè qua
        if (responsibilities_content == "Kh√¥ng t√¨m th·∫•y." and
                requirements_content == "Kh√¥ng t√¨m th·∫•y." and
                benefits_content == "Kh√¥ng t√¨m th·∫•y."):
            print(
                f"‚ö†Ô∏è B·ªè qua l∆∞u v√†o database cho '{job_title}' ({normalized_detail_url}). Kh√¥ng t√¨m th·∫•y n·ªôi dung M√¥ t·∫£ c√¥ng vi·ªác, Y√™u c·∫ßu c√¥ng vi·ªác, ho·∫∑c Quy·ªÅn l·ª£i.")
            print("-" * (len(normalized_detail_url) + 40))
            return

        # D·ªØ li·ªáu c√¥ng vi·ªác ƒë·ªÉ l∆∞u tr·ªØ
        job_data = {
            "url": normalized_detail_url,
            "title": job_title,
            "responsibilities": responsibilities_content,
            "requirements": requirements_content,
            "benefits": benefits_content,
            "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }

        # --- LOGIC KI·ªÇM TRA TR√ôNG L·∫∂P M·ªöI: N·∫øu tr√πng title HO·∫∂C responsibilities th√¨ KH√îNG TH√äM ---

        # 1. Ki·ªÉm tra xem c√≥ b·∫£n ghi n√†o tr√πng title kh√¥ng
        duplicate_by_title = collection.find_one({'title': job_data['title']})

        # 2. Ki·ªÉm tra xem c√≥ b·∫£n ghi n√†o tr√πng responsibilities kh√¥ng
        duplicate_by_responsibilities = collection.find_one({'responsibilities': job_data['responsibilities']})

        if duplicate_by_title or duplicate_by_responsibilities:
            # N·∫øu t√¨m th·∫•y tr√πng l·∫∑p theo title HO·∫∂C responsibilities
            duplicate_criteria = []
            if duplicate_by_title:
                duplicate_criteria.append("Ti√™u ƒë·ªÅ")
            if duplicate_by_responsibilities:
                duplicate_criteria.append("M√¥ t·∫£ c√¥ng vi·ªác")

            print(
                f"‚ö†Ô∏è Ph√°t hi·ªán tr√πng l·∫∑p theo {' HO·∫∂C '.join(duplicate_criteria)} cho '{job_data['title']}' ({job_data['url']}).")
            print(f"   B·ªè qua vi·ªác th√™m b·∫£n ghi n√†y ƒë·ªÉ tr√°nh tr√πng l·∫∑p n·ªôi dung.")
            print("-" * (len(normalized_detail_url) + 40))
            return  # D·ª´ng h√†m, kh√¥ng th√™m v√†o database
        else:
            # N·∫øu kh√¥ng c√≥ tr√πng l·∫∑p theo ti√™u ƒë·ªÅ HO·∫∂C m√¥ t·∫£ c√¥ng vi·ªác,
            # th√¨ ƒë√¢y l√† m·ªôt c√¥ng vi·ªác m·ªõi, ch√∫ng ta s·∫Ω th√™m n√≥ v√†o.
            try:
                # V·∫´n s·ª≠ d·ª•ng upsert d·ª±a tr√™n URL ƒë·ªÉ n·∫øu m·ªôt c√¥ng vi·ªác c√≥ c√πng URL nh∆∞ng kh√¥ng c√≥ n·ªôi dung
                # ƒë∆∞·ª£c thu th·∫≠p tr∆∞·ªõc ƒë√≥, n√≥ s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t.
                # Tuy nhi√™n, n·∫øu URL l√† ho√†n to√†n m·ªõi V√Ä n·ªôi dung kh√¥ng tr√πng l·∫∑p, n√≥ s·∫Ω ƒë∆∞·ª£c th√™m m·ªõi.
                result = collection.update_one(
                    {'url': job_data['url']},  # Filter by URL to ensure URL uniqueness (if index is active)
                    {'$set': job_data},
                    upsert=True  # Ch√®n n·∫øu kh√¥ng t√¨m th·∫•y URL, ho·∫∑c c·∫≠p nh·∫≠t n·∫øu t√¨m th·∫•y
                )

                if result.upserted_id:
                    print(f"‚úÖ ƒê√£ th√™m m·ªõi d·ªØ li·ªáu cho '{job_data['title']}' v√†o MongoDB (ID: {result.upserted_id}).")
                elif result.modified_count > 0:
                    print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t d·ªØ li·ªáu cho '{job_data['title']}' trong MongoDB (d·ªØ li·ªáu URL ƒë√£ t·ªìn t·∫°i).")
                else:
                    print(f"‚ÑπÔ∏è D·ªØ li·ªáu cho '{job_data['title']}' ƒë√£ t·ªìn t·∫°i v√† kh√¥ng c√≥ thay ƒë·ªïi.")

            except Exception as e:
                # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p URL tr√πng l·∫∑p n·∫øu ch·ªâ m·ª•c URL duy nh·∫•t ƒë∆∞·ª£c b·∫≠t v√† b·∫°n c·ªë g·∫Øng ch√®n
                # m·ªôt URL ƒë√£ c√≥ m√† update_one kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c (√≠t kh·∫£ nƒÉng x·∫£y ra v·ªõi upsert=True)
                if "E11000 duplicate key error" in str(e) and "url" in str(e):
                    print(f"‚ÑπÔ∏è D·ªØ li·ªáu cho '{job_data['title']}' ({job_data['url']}) ƒë√£ t·ªìn t·∫°i theo URL.")
                else:
                    print(f"‚ùå L·ªói khi l∆∞u d·ªØ li·ªáu v√†o MongoDB cho '{job_data['title']}': {e}")

        # --- K·∫æT TH√öC LOGIC KI·ªÇM TRA TR√ôNG L·∫∂P M·ªöI ---

        print("\n" + "=" * 50)
        print(f"D·ªÆ LI·ªÜU TR√çCH XU·∫§T T·ª™ TRANG CHI TI·∫æT ({normalized_detail_url}):")
        print("=" * 50 + "\n")
        print(f"Ti√™u ƒë·ªÅ: {job_title}")
        print(f"1. M√¥ t·∫£ c√¥ng vi·ªác: {responsibilities_content}")
        print(f"2. Y√™u c·∫ßu c√¥ng vi·ªác: {requirements_content}")
        print(f"3. Quy·ªÅn l·ª£i: {benefits_content}")
        print("\n" + "=" * 50)

    else:
        print(f"Kh√¥ng th·ªÉ t·∫£i n·ªôi dung HTML c·ªßa trang chi ti·∫øt: {normalized_detail_url}")
    print("-" * (len(normalized_detail_url) + 40))


if __name__ == "__main__":
    client = None
    try:
        client = MongoClient(MONGO_URI)
        client.admin.command('ping')
        print(f"‚úÖ ƒê√£ k·∫øt n·ªëi th√†nh c√¥ng t·ªõi MongoDB Atlas cluster.")
        db = client[DATABASE_NAME]
        collection = db[COLLECTION_NAME]
        print(f"ƒêang s·ª≠ d·ª•ng database '{DATABASE_NAME}' v√† collection '{COLLECTION_NAME}'.")

        # ƒê·∫£m b·∫£o c√°c ch·ªâ m·ª•c ƒë∆∞·ª£c thi·∫øt l·∫≠p
        # CH·ªà ƒê·ªÇ L·∫†I CH·ªà M·ª§C DUY NH·∫§T TR√äN 'url' ƒë·ªÉ ngƒÉn URL b·ªã tr√πng l·∫∑p.
        # Ch√∫ng ta KH√îNG T·∫†O ch·ªâ m·ª•c duy nh·∫•t tr√™n 'title' HO·∫∂C 'responsibilities'
        # v√¨ logic "OR" ƒë∆∞·ª£c x·ª≠ l√Ω trong m√£ Python.
        try:
            collection.create_index("url", unique=True)
            print("‚úÖ ƒê√£ ƒë·∫£m b·∫£o ch·ªâ m·ª•c duy nh·∫•t tr√™n tr∆∞·ªùng 'url'.")
        except Exception as e:
            print(f"‚ÑπÔ∏è L·ªói khi t·∫°o ch·ªâ m·ª•c duy nh·∫•t tr√™n 'url' (c√≥ th·ªÉ ƒë√£ t·ªìn t·∫°i): {e}")

        # # N·∫øu b·∫°n ƒë√£ t·∫°o c√°c ch·ªâ m·ª•c duy nh·∫•t kh√°c tr√™n title/responsibilities tr∆∞·ªõc ƒë√≥,
        # # b·∫°n N√äN x√≥a ch√∫ng ƒë·ªÉ tr√°nh xung ƒë·ªôt v·ªõi logic m·ªõi. V√≠ d·ª•:
        # try:
        #     collection.drop_index("title_1")
        #     collection.drop_index("responsibilities_1")
        #     collection.drop_index("title_1_responsibilities_1") # N·∫øu c√≥ ch·ªâ m·ª•c t·ªïng h·ª£p
        #     print("‚ÑπÔ∏è ƒê√£ x√≥a c√°c ch·ªâ m·ª•c duy nh·∫•t c≈© tr√™n title/responsibilities n·∫øu c√≥.")
        # except Exception as e:
        #     pass # B·ªè qua n·∫øu ch·ªâ m·ª•c kh√¥ng t·ªìn t·∫°i

        initial_listing_url = "https://vieclam24h.vn/viec-lam-it-phan-mem-o8.html?occupation_ids[]=8&sort_q=priority_max%2Cdesc"

        all_detail_links = set()
        page_number = 1
        max_pages_to_crawl = 3

        print(f"\n--- B·∫ÆT ƒê·∫¶U CRAWL DANH S√ÅCH VI·ªÜC L√ÄM ---")

        while page_number <= max_pages_to_crawl:
            parsed_url = urlparse(initial_listing_url)
            query_params = parse_qs(parsed_url.query)
            query_params['page'] = [str(page_number)]
            new_query_string = urlencode(query_params, doseq=True)
            current_page_url = urlunparse(parsed_url._replace(query=new_query_string))

            print(f"\nüîé ƒêang truy c·∫≠p trang danh s√°ch: {current_page_url}")

            html_content = get_html_content(current_page_url)

            if html_content:
                detail_links_on_page = extract_detail_links(html_content)

                if not detail_links_on_page:
                    print(
                        f"Kh√¥ng t√¨m th·∫•y li√™n k·∫øt chi ti·∫øt n√†o tr√™n trang {page_number}. C√≥ th·ªÉ ƒë√£ h·∫øt trang ho·∫∑c c·∫•u tr√∫c thay ƒë·ªïi.")
                    break

                new_links_count_on_page = 0
                for link in detail_links_on_page:
                    normalized_link = normalize_url(link)
                    if normalized_link not in all_detail_links:
                        all_detail_links.add(normalized_link)
                        new_links_count_on_page += 1

                print(
                    f"T√¨m th·∫•y {len(detail_links_on_page)} li√™n k·∫øt tr√™n trang {page_number}. Th√™m {new_links_count_on_page} li√™n k·∫øt m·ªõi v√†o danh s√°ch t·ªïng.")

                if new_links_count_on_page == 0 and page_number > 1:
                    print(
                        f"Kh√¥ng t√¨m th·∫•y li√™n k·∫øt m·ªõi n√†o tr√™n trang {page_number}. C√≥ th·ªÉ ƒë√£ thu th·∫≠p t·∫•t c·∫£ ho·∫∑c ƒë·∫øn cu·ªëi.")
                    break

                page_number += 1
                time.sleep(random.uniform(2, 5))
            else:
                print(
                    f"Kh√¥ng th·ªÉ l·∫•y n·ªôi dung HTML t·ª´ '{current_page_url}' ƒë·ªÉ ph√¢n t√≠ch li√™n k·∫øt. D·ª´ng l·∫°i qu√° tr√¨nh crawl danh s√°ch.")
                break

        print(f"\n--- K·∫æT TH√öC CRAWL DANH S√ÅCH VI·ªÜC L√ÄM ---")

        if all_detail_links:
            print(
                f"\nT·ªïng s·ªë {len(all_detail_links)} li√™n k·∫øt chi ti·∫øt duy nh·∫•t ƒë√£ ƒë∆∞·ª£c t√¨m th·∫•y qua t·∫•t c·∫£ c√°c trang.")
            links_to_process = list(all_detail_links)
            random.shuffle(links_to_process)

            max_details_to_process = 10
            if len(links_to_process) > max_details_to_process:
                print(
                    f"Ch·ªâ x·ª≠ l√Ω {max_details_to_process} li√™n k·∫øt chi ti·∫øt ƒë·∫ßu ti√™n trong s·ªë {len(links_to_process)} t√¨m th·∫•y (thay ƒë·ªïi 'max_details_to_process' ƒë·ªÉ ƒëi·ªÅu ch·ªânh).")
                links_to_process = links_to_process[:max_details_to_process]

            for i, link in enumerate(links_to_process):
                print(f"\n({i + 1}/{len(links_to_process)}) ")
                process_detail_link(link, collection)
                time.sleep(random.uniform(1, 3))

        else:
            print("Kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ li√™n k·∫øt chi ti·∫øt n√†o.")
    except Exception as e:
        print(f"‚ùå L·ªói t·ªïng qu√°t trong qu√° tr√¨nh crawl ho·∫∑c k·∫øt n·ªëi MongoDB: {e}")
    finally:
        if client:
            client.close()
            print("\nüëã ƒê√£ ƒë√≥ng k·∫øt n·ªëi MongoDB.")

    print("\nCh∆∞∆°ng tr√¨nh ƒë√£ ho√†n t·∫•t.")