import requests
import time
import random  # ƒê·ªÉ ng·∫´u nhi√™n h√≥a th·ªùi gian ch·ªù
from bs4 import BeautifulSoup
from pymongo import MongoClient  # Import th∆∞ vi·ªán PyMongo
import re  # Import re module ƒë·ªÉ l√†m s·∫°ch chu·ªói

# --- C·∫•u h√¨nh MongoDB ---
# ƒê·∫£m b·∫£o MongoDB server ƒëang ch·∫°y tr√™n m√°y c·ªßa b·∫°n (m·∫∑c ƒë·ªãnh l√† localhost:27017)
# ƒê√É C·∫¨P NH·∫¨T CHU·ªñI K·∫æT N·ªêI MONGODB THEO Y√äU C·∫¶U C·ª¶A B·∫†N
MONGO_URI = "mongodb+srv://lumconon0911:SWD-SWD@cluster.slolqwf.mongodb.net/"
DATABASE_NAME = "topdev_jobs_db"  # T√™n database cho d·ªØ li·ªáu TopDev
COLLECTION_NAME = "job_details"  # T√™n collection ƒë·ªÉ l∆∞u chi ti·∫øt c√¥ng vi·ªác


def has_all_classes(tag, class_string):
    """
    Checks if a BeautifulSoup tag has all classes specified in a space-separated string.

    Args:
        tag (bs4.Tag): The BeautifulSoup tag to check.
        class_string (str): A space-separated string of classes to look for.

    Returns:
        bool: True if the tag has all specified classes, False otherwise.
    """
    if not tag or not tag.has_attr('class'):
        return False
    target_classes_list = class_string.split()
    # Check if all target classes are present in the tag's class list
    return all(cls in tag['class'] for cls in target_classes_list)


def get_html_content(url):
    """
    T·∫£i xu·ªëng n·ªôi dung HTML c·ªßa m·ªôt trang web v√† tr·∫£ v·ªÅ n√≥.

    Args:
        url (str): ƒê·ªãa ch·ªâ URL c·ªßa trang web c·∫ßn t·∫£i.

    Returns:
        str: N·ªôi dung HTML c·ªßa trang web, ho·∫∑c None n·∫øu c√≥ l·ªói.
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)  # Added timeout
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"L·ªói khi t·∫£i trang web '{url}': {e}")
        return None


def extract_detail_links(html_content):
    """
    Ph√¢n t√≠ch c√∫ ph√°p n·ªôi dung HTML ƒë·ªÉ tr√≠ch xu·∫•t c√°c li√™n k·∫øt chi ti·∫øt
    t·ª´ m·ªôt container c·ª• th·ªÉ v√† lo·∫°i b·ªè tr√πng l·∫∑p.

    Args:
        html_content (str): N·ªôi dung HTML c·ªßa trang web.

    Returns:
        set: M·ªôt t·∫≠p h·ª£p c√°c URL chi ti·∫øt duy nh·∫•t.
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    unique_detail_links = set()

    # The new target class for the listing page container
    target_classes_listing_page = "grid grid-cols-1 gap-y-2 lg:gap-y-2.5"

    # Use lambda with has_all_classes to robustly find the container div
    container = soup.find('div',
                          class_=lambda c: has_all_classes(BeautifulSoup(f'<div class="{c}"></div>', 'html.parser').div,
                                                           target_classes_listing_page) if c else False)

    if container:
        links = container.find_all('a', href=True)
        for link in links:
            href = link['href']
            # Ensure links are absolute. Base URL for vieclam24h.vn.
            if href.startswith('http'):
                unique_detail_links.add(href)
            else:
                base_url = "https://vieclam24h.vn"
                full_link = f"{base_url}{href}"
                unique_detail_links.add(full_link)
    else:
        print(f"Kh√¥ng t√¨m th·∫•y container v·ªõi c√°c class: '{target_classes_listing_page}'.")

    return unique_detail_links


def extract_section_content(soup_obj, heading_text):
    """
    T√¨m m·ªôt ti√™u ƒë·ªÅ (h2, h3, ho·∫∑c span v·ªõi class c·ª• th·ªÉ) v√† tr√≠ch xu·∫•t n·ªôi dung t·ª´ c√°c ph·∫ßn t·ª≠ k·∫ø ti·∫øp.
    T√¨m ki·∫øm kh√¥ng ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng cho heading_text.
    """
    # Convert the search heading text to lowercase for case-insensitive comparison
    lower_heading_text = heading_text.lower()

    heading_tag = None

    # Try to find heading in h2, h3, or span
    for tag_name in ['h2', 'h3', 'span']:
        heading_tag = soup_obj.find(tag_name, string=lambda text: text and lower_heading_text in text.lower())
        if heading_tag:
            break  # Found a heading, exit loop

    if heading_tag:
        content_element = None

        # User-specified class for content sections that often contain the details
        specific_content_class = "jsx-5b2773f86d2f74b mb-2 text-14 break-words text-se-neutral-80 text-description"

        # Priority 1: Find div with class "prose" as next sibling (common pattern)
        content_element = heading_tag.find_next_sibling('div', class_='prose')

        # Priority 2: If not found, try the specific user-provided class for div/span as next sibling
        if not content_element:
            # Need to search for both div and span with this complex class
            for tag_type in ['div', 'span']:
                # Create a dummy tag to use has_all_classes helper
                dummy_tag_html = f'<{tag_type} class="">content</{tag_type}>'

                potential_content = heading_tag.find_next_sibling(tag_type, class_=lambda c: has_all_classes(
                    BeautifulSoup(dummy_tag_html, 'html.parser').find(tag_type),
                    specific_content_class) if c else False)
                if potential_content:
                    content_element = potential_content
                    break  # Found it, break from tag_type loop

        # Priority 3: If still not found, try common list/paragraph/div/span containers as next sibling
        if not content_element:
            content_element = heading_tag.find_next_sibling(['div', 'ul', 'ol', 'p', 'span'])

        if content_element:
            # Get raw text, replace newlines with space, then normalize all whitespace
            raw_text = content_element.get_text(separator=' ', strip=False)
            cleaned_text = re.sub(r'\s+', ' ', raw_text).strip()
            return cleaned_text
        else:
            # Fallback: If no suitable sibling element is found,
            # return the text of the heading itself (if it contains more than just the heading text)
            raw_heading_text = heading_tag.get_text(separator=' ', strip=False)
            cleaned_heading_text = re.sub(r'\s+', ' ', raw_heading_text).strip()

            # If the cleaned heading text is just the heading itself, means no distinct content was found
            if cleaned_heading_text.lower() == lower_heading_text:
                return None  # Return None if no actual content found, making it cleaner for checks
            else:
                return cleaned_heading_text

    return None  # Return None if heading not found.


def crawl_topdev_simple():
    """
    Th·ª±c hi·ªán crawl trang danh s√°ch vi·ªác l√†m IT tr√™n TopDev.vn,
    tr√≠ch xu·∫•t URL chi ti·∫øt, truy c·∫≠p t·ª´ng URL v√† l∆∞u v√†o MongoDB.
    Kh√¥ng s·ª≠ d·ª•ng GPMlogin, d·ªãch v·ª• anti-captcha.
    """
    client = None  # Kh·ªüi t·∫°o client b√™n ngo√†i ƒë·ªÉ ƒë·∫£m b·∫£o ƒë√≥ng k·∫øt n·ªëi
    try:
        # K·∫øt n·ªëi ƒë·∫øn MongoDB
        client = MongoClient(MONGO_URI)
        # Ping the server to ensure a successful connection
        client.admin.command('ping')
        print(f"‚úÖ ƒê√£ k·∫øt n·ªëi th√†nh c√¥ng t·ªõi MongoDB Atlas cluster.")
        db = client[DATABASE_NAME]
        collection = db[COLLECTION_NAME]
        print(f"ƒêang s·ª≠ d·ª•ng database '{DATABASE_NAME}' v√† collection '{COLLECTION_NAME}'.")

        list_url = "https://topdev.vn/jobs/search?job_categories_ids=2"
        print(f"üîé ƒêang truy c·∫≠p trang danh s√°ch: {list_url}")

        # C·∫•u h√¨nh User-Agent ƒë·ªÉ gi·ªëng tr√¨nh duy·ªát th·∫≠t h∆°n
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

        try:
            response = requests.get(list_url, headers=headers, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"‚ùå L·ªói khi truy c·∫≠p trang danh s√°ch {list_url}: {e}")
            return

        list_soup = BeautifulSoup(response.text, 'html.parser')

        job_detail_urls = []
        job_item_containers = list_soup.find_all('div',
                                                 class_='relative rounded border border-solid transition-all hover:shadow-md border-primary bg-primary-100')

        if not job_item_containers:
            print("‚ùå Kh√¥ng t√¨m th·∫•y container vi·ªác l√†m n√†o tr√™n trang danh s√°ch v·ªõi b·ªô ch·ªçn ƒë√£ ƒë·ªãnh.")
            return

        print(f"ƒê√£ t√¨m th·∫•y {len(job_item_containers)} container vi·ªác l√†m tr√™n trang danh s√°ch.")

        for container in job_item_containers:
            parent_a_tag = container.find_parent('a', href=True)
            if parent_a_tag:
                detail_url = parent_a_tag.get('href')
                if detail_url:
                    if not detail_url.startswith('http'):
                        detail_url = f"https://topdev.vn{detail_url}"
                    job_detail_urls.append(detail_url)

        job_detail_urls = list(set(job_detail_urls))

        if not job_detail_urls:
            print("‚ùå Kh√¥ng t√¨m th·∫•y URL chi ti·∫øt vi·ªác l√†m n√†o sau khi tr√≠ch xu·∫•t v√† l·ªçc tr√πng l·∫∑p.")
            return

        print(f"ƒê√£ tr√≠ch xu·∫•t {len(job_detail_urls)} URL chi ti·∫øt (ƒë√£ lo·∫°i tr√πng l·∫∑p).")

        # --- L·∫∂P QUA T·ª™NG URL CHI TI·∫æT V√Ä TR√çCH XU·∫§T, L∆ØU V√ÄO MONGODB ---
        for i, detail_url in enumerate(job_detail_urls):
            print(f"\n--- [{i + 1}/{len(job_detail_urls)}] ƒêang truy c·∫≠p trang chi ti·∫øt: {detail_url} ---")
            try:
                detail_response = requests.get(detail_url, headers=headers, timeout=10)
                detail_response.raise_for_status()

                detail_soup = BeautifulSoup(detail_response.text, 'html.parser')

                # Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ c√¥ng vi·ªác (v√≠ d·ª• t·ª´ th·∫ª <title> c·ªßa trang)
                job_title = detail_soup.find('title').get_text(strip=True) if detail_soup.find(
                    'title') else "Kh√¥ng c√≥ ti√™u ƒë·ªÅ."

                # Tr√≠ch xu·∫•t n·ªôi dung cho t·ª´ng ph·∫ßn
                # Attempt to get content, which will be None if not found
                responsibilities_content = extract_section_content(detail_soup,
                                                                   "Responsibilities") or extract_section_content(
                    detail_soup, "Tr√°ch nhi·ªám c√¥ng vi·ªác")
                requirements_content = extract_section_content(detail_soup, "Requirements") or extract_section_content(
                    detail_soup, "K·ªπ nƒÉng & Chuy√™n m√¥n")
                benefits_content = extract_section_content(detail_soup, "Benefits") or extract_section_content(
                    detail_soup, "ph√∫c l·ª£i d√†nh cho b·∫°n")

                # --- KI·ªÇM TRA ƒêI·ªÄU KI·ªÜN L∆ØU V√ÄO DATABASE ---
                # Ch·ªâ l∆∞u v√†o database n·∫øu √≠t nh·∫•t m·ªôt trong ba ph·∫ßn ch√≠nh c√≥ n·ªôi dung
                if not responsibilities_content and not requirements_content and not benefits_content:
                    print(
                        f"‚ö†Ô∏è B·ªè qua l∆∞u v√†o database cho '{job_title}' ({detail_url}). Kh√¥ng t√¨m th·∫•y n·ªôi dung Responsibilities, Requirements, ho·∫∑c Benefits.")
                    time.sleep(random.uniform(1, 3))  # V·∫´n t·∫°m d·ª´ng ƒë·ªÉ tr√°nh qu√° t·∫£i
                    continue  # Chuy·ªÉn sang URL ti·∫øp theo

                # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ l∆∞u v√†o MongoDB
                job_data = {
                    "url": detail_url,
                    "title": job_title,
                    "responsibilities": responsibilities_content,  # S·∫Ω l√† None n·∫øu kh√¥ng t√¨m th·∫•y
                    "requirements": requirements_content,  # S·∫Ω l√† None n·∫øu kh√¥ng t√¨m th·∫•y
                    "benefits": benefits_content,  # S·∫Ω l√† None n·∫øu kh√¥ng t√¨m th·∫•y
                    "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S")  # Th·ªùi gian crawl
                }

                # L∆∞u d·ªØ li·ªáu v√†o MongoDB (upsert ƒë·ªÉ tr√°nh tr√πng l·∫∑p)
                # N·∫øu 'url' ƒë√£ t·ªìn t·∫°i, c·∫≠p nh·∫≠t b·∫£n ghi ƒë√≥. N·∫øu kh√¥ng, ch√®n b·∫£n ghi m·ªõi.
                result = collection.update_one(
                    {'url': job_data['url']},  # Filter ƒë·ªÉ t√¨m b·∫£n ghi
                    {'$set': job_data},  # D·ªØ li·ªáu ƒë·ªÉ c·∫≠p nh·∫≠t/ch√®n
                    upsert=True  # N·∫øu kh√¥ng t√¨m th·∫•y, t·∫°o m·ªõi
                )

                if result.upserted_id:
                    print(f"‚úÖ ƒê√£ th√™m m·ªõi d·ªØ li·ªáu cho '{job_data['title']}' v√†o MongoDB (ID: {result.upserted_id}).")
                elif result.modified_count > 0:
                    print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t d·ªØ li·ªáu cho '{job_data['title']}' trong MongoDB.")
                else:
                    print(f"‚ÑπÔ∏è D·ªØ li·ªáu cho '{job_data['title']}' ƒë√£ t·ªìn t·∫°i v√† kh√¥ng c√≥ thay ƒë·ªïi.")

                # In d·ªØ li·ªáu ƒë√£ tr√≠ch xu·∫•t ra console ƒë·ªÉ ki·ªÉm tra
                print("\n" + "=" * 50)
                print(f"D·ªÆ LI·ªÜU TR√çCH XU·∫§T T·ª™ TRANG CHI TI·∫æT ({detail_url}):")
                print("=" * 50 + "\n")
                print(f"Ti√™u ƒë·ªÅ: {job_title}")
                # Hi·ªÉn th·ªã "Kh√¥ng t√¨m th·∫•y." n·∫øu n·ªôi dung l√† None
                print(
                    f"1. Responsibilities: {responsibilities_content if responsibilities_content else 'Kh√¥ng t√¨m th·∫•y.'}")
                print(f"2. Requirements: {requirements_content if requirements_content else 'Kh√¥ng t√¨m th·∫•y.'}")
                print(f"3. Benefits: {benefits_content if benefits_content else 'Kh√¥ng t√¨m th·∫•y.'}")
                print("\n" + "=" * 50)

                # T·∫°m d·ª´ng ng·∫´u nhi√™n ƒë·ªÉ tr√°nh b·ªã ch·∫∑n IP
                time.sleep(random.uniform(1, 3))
            except requests.exceptions.RequestException as e:
                print(f"‚ùå L·ªói khi truy c·∫≠p trang chi ti·∫øt {detail_url}: {e}")
                continue
            except Exception as e:
                print(f"‚ùå L·ªói khi x·ª≠ l√Ω d·ªØ li·ªáu ho·∫∑c l∆∞u v√†o MongoDB cho {detail_url}: {e}")
                continue

    except Exception as e:
        print(f"‚ùå L·ªói t·ªïng qu√°t trong qu√° tr√¨nh crawl ho·∫∑c k·∫øt n·ªëi MongoDB: {e}")
    finally:
        if client:
            client.close()
            print("\nüëã ƒê√£ ƒë√≥ng k·∫øt n·ªëi MongoDB.")

    print("\nHo√†n t·∫•t qu√° tr√¨nh crawl c√°c link chi ti·∫øt v√† l∆∞u d·ªØ li·ªáu v√†o MongoDB.")


if __name__ == "__main__":
    crawl_topdev_simple()
    print("\nCh∆∞∆°ng tr√¨nh ƒë√£ ho√†n t·∫•t.")
